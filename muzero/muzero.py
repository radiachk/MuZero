from muzero_config import MuZeroConfig
from shared_network_storage import SharedNetworkStorage
from replay_buffer import ReplayBuffer

# TODO: Klasse muss angepasst werdem
"""
The class representing the entiry MuZero structure. 
It seperates the process of generating the training data from the actual "playing" in order to avoid chasing a moving target.
The network structure constists of three seperate models: Representation Model, Dynamics Model and the Prediction Model.
Each training period starts with a new network, which is trained on the training data "improved" on each generation period.
"""
class MuZero(object):

    def __init__(self, config: MuZeroConfig):
        self.network_storage = NetworkStorage()                                     # Storage containing all saved networks (generating the training data) 
        self.replay_buffer = ReplayBuffer(config)                                   # The replay buffer containing the matches to train on
        self.config = config                                                        # Configuration file

    """
    Use the most recent version of the network to generate the trainings data and save it into the replay buffer
    """
    def populate_replay_buffer(self):
        network = storage.latest_network()                                          
        for match in range(self.config.window_size):                               
            game = play_game(config, network)
            replay_buffer.save_game(game)

    """
    Create a new network and train it on the data generated by the self-play
    """
    def train_network(self):
        # Create new network (representaion model, dynamics model, prediction model)
        network = Network()
        # Set the learning rate accordingly to the current training step
        learning_rate = self.config.lr_init * self.config.lr_decay_rate**(
            tf.train.get_global_step() / self.config.lr_decay_steps)
        # Optimizer is the SGD optimizer with momentum (TODO: check if this optimzer uses stochastic gradient descent)
        optimizer = tf.train.MomentumOptimizer(learning_rate, self.config.momentum)

        for step in range(self.config.training_steps):
            # Save the current state of the network oon
            if step % self.config.checkpoint_interval == 0:
                self.network_storage.save_network(step, network)

            # Sample a batch from the replay buffer
            batch = replay_buffer.sample_batch(self.config.num_unroll_steps, self.config.td_steps)

            # Calculate the loss that results from the batch and update the weights accordingly
            update_weights(optimizer, network, batch, self.config.weight_decay)
        
        # Finally save the trained network
        self.network_storage.save_network(self.config.training_steps, network)

    """
    First predict the values for the given observations and actions 
    Then calculate the loss between those predictions and results of the MCTS stored in the replay buffer 
    Before optimizing the weights, add a L2-Regularization to the loss (adds a penalty if the weights get to big; prevents overfitting)
    Finally updating all weights in the network (all 3 models) with the given optimizer.
    """ 
    def update_weights(optimizer: tf.train.Optimizer, network: Network, batch, weight_decay: float):
        loss = 0
        for image, actions, targets in batch:
            # Initial step. Convert the real observation into a hidden state, then predict value, reward and policy logits (propability distribution) on it
            # TODO: Dont understand how the reward can be predicted on a single state (without any given action)
            value, reward, policy_logits, hidden_state = network.initial_inference(image)
            predictions = [(1.0, value, reward, policy_logits)]

            # Recurrent steps, from action and previous hidden state.
            for action in actions:
                value, reward, policy_logits, hidden_state = network.recurrent_inference(hidden_state, action)
                hidden_state = tf.scale_gradient(hidden_state, 0.5)
                predictions.append((1.0 / len(actions), value, reward, policy_logits))

            # Calculate the loss between the predictions and the target
            for prediction, target in zip(predictions, targets):
                gradient_scale, value, reward, policy_logits = prediction
                target_value, target_reward, target_policy = target

                l = (
                    scalar_loss(value, target_value) +
                    scalar_loss(reward, target_reward) +
                    tf.nn.softmax_cross_entropy_with_logits(logits=policy_logits, labels=target_policy)
                    )

                loss += tf.scale_gradient(l, gradient_scale)

        # Add L2-Regularization to the overall loss
        for weights in network.get_weights():
            loss += weight_decay * tf.nn.l2_loss(weights)

        # Optimize the current weights
        optimizer.minimize(loss)

    """
    Calculating the scalar loss between prediction and the target.
    MSE in board games
    Cross entropy between categorical values in Atari
    TODO: Finish function
    """
    def scalar_loss(prediction, target) -> float:
        return -1

    # TODO: Finish function
    def play_game(self, config: MuZeroConfig, network: Network):
        return 1

