from muzero.network.muzero_config import MuZeroConfig, MuZeroBoardConfig, MuZeroAtariConfig
from muzero.network.network_storage import NetworkStorage
from muzero.network.replay_buffer import ReplayBuffer
from muzero.network.network import Network
from muzero.mcts.tree import Tree
from muzero.environment.games import Game
from muzero.environment.player import Player

import tensorflow as tf
from tensorflow.keras.optimizers import Optimizer, SGD
import multiprocessing
from datetime import datetime
from multiprocessing import Pool, Process
import time
import os


class MuZero:
    """
    The class representing the entire MuZero structure.
    Separates the process of generating training data from the actual "playing" in order to avoid chasing a moving target.
    The network structure consists of three separate models: Representation Model, Dynamics Model and the Prediction Model.
    Each training period starts with a new network, which is trained on training data "improved" on each generation period.
    """

    def __init__(self, config: MuZeroConfig):
        self.network_storage = NetworkStorage(config)  # Storage containing all saved networks
        self.replay_buffer = ReplayBuffer(config)  # The replay buffer containing the matches to train on
        self.config = config  # Configuration file
        self.frame_count = 0
        self.logdir = 'logs/' + datetime.now().strftime("%Y%m%d-%H%M%S")
        self.graph_traced = False
        self.writer = None

    def start_training(self):
        """
        Use the most recent version of the network to generate the trainings data and save it into the replay buffer
        """
        p = Process(target=self.train_network, args=[])
        p.start()
        num_cores = multiprocessing.cpu_count() - 4
        pool_selfplay = Pool(processes=num_cores)
        pool_selfplay.apply_async(self.run_selfplay())
        p.join()

    def run_selfplay(self):
        """
        Each of the parallel processes is continuously generating new training data
        """
        print("Selfplay worker started")
        while True:
            network = self.network_storage.latest_network()
            game = self.play_game(network)
            self.replay_buffer.save_game(game)
            if len(self.replay_buffer) % self.config.buffer_save_game_interval == 0:
                self.replay_buffer.save()

    def play_game(self, network: Network) -> Game:
        """
        Each of the games produced by a process is played using the latest network,
        so that quality of training data should increase very quick at the beginning
        """
        player_helper = 0
        game = self.config.new_game()
        tree = Tree(action_list=game.legal_actions(),
                    config=self.config,
                    network=network,
                    player_list=game.players,
                    discount=self.config.discount)

        while not game.terminal() and len(game.root_values) < self.config.max_moves:
            self.frame_count += 1
            image = game.make_image(-1)
            value, reward, policy_logits, hidden_state = network.initial_inference(image)

            tree.reset(value=value, reward=reward, policy_logits=policy_logits, hidden_state=hidden_state)
            action = tree.get_action(evaluation=False)

            game.apply(action, Player(player_helper % len(game.players)))
            game.store_search_statistics(tree.root)
        return game

    def train_network(self):
        """
        Create a new network and train it on the data generated by the self-play

        :return: A integer to make it possible to await this method
        """
        print("Started training job but waits till buffer is filled enough")
        # Create new network (representation model, dynamics model, prediction model)
        network = Network(num_action=self.config.action_space_size, game_mode='Atari')
        learning_rate = self.config.lr_init * self.config.lr_decay_rate ** (
                network.training_steps() / self.config.lr_decay_steps)
        opt = SGD(learning_rate=learning_rate, momentum=self.config.momentum)

        while len(self.replay_buffer.buffer) < self.config.buffer_save_game_interval:
            time.sleep(5)

        print("Training started")

        for step in range(self.config.training_steps):
            # Save the current state of the network
            if step % self.config.checkpoint_interval == 0:
                self.network_storage.save_network(step, network)

            if len(self.replay_buffer) % self.config.buffer_save_game_interval == 0:
                self.replay_buffer.load()

            # Sample a batch from the replay buffer
            batch = self.replay_buffer.sample_batch(self.config.num_unroll_steps, self.config.td_steps)

            # Calculate the loss that results from the batch and update the weights accordingly
            self.update_weights(opt, network, batch, self.config.weight_decay)

        # Finally save the trained network
        self.network_storage.save_network(self.config.training_steps, network)

    def update_weights(self, optimizer: Optimizer, network: Network, batch, weight_decay: float):
        """
        First predict the values for the given observations and actions
        Then calculate the loss between those predictions and results of the MCTS stored in the replay buffer
        Before optimizing the weights, add a L2-Regularization to the loss (adds a penalty if the weights get to big)
        Finally updating all weights in the network (all 3 models) with the given optimizer.
        """

        def scale_gradient(tensor, scale):
            return tensor * scale + tf.stop_gradient(tensor) * (1. - scale)

        def get_loss_callback():
            loss = tf.convert_to_tensor([0], dtype=float)

            for image, actions, targets in batch:
                # Prevents the computation of the image to be taken into account for computing gradients
                image = tf.stop_gradient(image, 'get_image')
                # Transform real observation to hidden state, then predict value, reward and policy distribution for it

                value, reward, policy_logits, hidden_state = network.initial_inference(image)
                predictions = [(1.0, value, reward, policy_logits)]

                # Recurrent steps, from action and previous hidden state.
                for action in actions:
                    # Build the input for the recurrent inference and stop the gradients at the input
                    action_tensor = tf.convert_to_tensor(action.action_id, dtype=float)
                    action_layer = tf.ones(hidden_state.shape) * action_tensor
                    hidden_state_with_action = tf.concat([hidden_state, action_layer], axis=3)
                    hidden_state_with_action = tf.stop_gradient(hidden_state_with_action, 'get_hidden_state_with_action')

                    value, reward, policy_logits, hidden_state = network.recurrent_inference(hidden_state_with_action)

                    hidden_state = scale_gradient(hidden_state, 0.5)
                    predictions.append((1.0 / len(actions), value, reward, policy_logits))

                # Calculate the loss between the predictions and the target
                for prediction, target in zip(predictions, targets):
                    gradient_scale, value, reward, policy_logits = prediction
                    target_value, target_reward, target_policy = target

                    assert value.shape == target_value.shape
                    assert reward.shape == target_reward.shape
                    assert policy_logits.shape == target_policy.shape

                    l = (
                            self.scalar_loss(value, target_value) +
                            self.scalar_loss(reward, target_reward) +
                            self.scalar_loss(policy_logits, target_policy)
                    )

                    loss += scale_gradient(l, gradient_scale)

            print('Loss before l2: ', loss)

            """
            FIXME: L2 loss is way to high (sometimes even inf) 
            # Add L2-Regularization to the overall loss
            for weights in network.get_weights_callback()():
                loss += weight_decay * tf.nn.l2_loss(weights)
            """

            # Optimize the current weights
            print('Loss after l2: ', loss)

            return loss

        optimizer.minimize(get_loss_callback, network.get_weights_callback())
        network.train_step += 1

    def scalar_loss(self, prediction, target) -> float:
        """
        Calculating the scalar loss between prediction and the target and returning it as a tensor.
        MSE in board games
        TODO: Finish cross entropy for atari games with categorical values
        """
        if isinstance(self.config, MuZeroBoardConfig):
            squared_error = tf.keras.losses.MSE(prediction, target)
            return tf.cast(squared_error, dtype=float)

        elif isinstance(self.config, MuZeroAtariConfig):
            squared_error = tf.keras.losses.MSE(prediction, target)
            return tf.cast(squared_error, dtype=float)
        else:
            raise Exception('MuZero', 'Please use the AtariConfig or the BoardConfig for calculating appropriate loss')
