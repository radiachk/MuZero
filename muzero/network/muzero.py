from muzero.network.muzero_config import MuZeroConfig, MuZeroBoardConfig, MuZeroAtariConfig
from muzero.network.network_storage import NetworkStorage
from muzero.network.replay_buffer import ReplayBuffer
from muzero.network.network import Network
from muzero.mcts.tree import Tree
from muzero.environment.games import Game
from muzero.environment.player import Player

import tensorflow as tf
import multiprocessing
from multiprocessing import Pool, Process
import numpy as np
import time
import os


class MuZero:
    """
    The class representing the entire MuZero structure.
    Separates the process of generating training data from the actual "playing" in order to avoid chasing a moving target.
    The network structure consists of three separate models: Representation Model, Dynamics Model and the Prediction Model.
    Each training period starts with a new network, which is trained on training data "improved" on each generation period.
    """

    def __init__(self, config: MuZeroConfig):
        self.network_storage = NetworkStorage(config)  # Storage containing all saved networks
        self.replay_buffer = ReplayBuffer(config)  # The replay buffer containing the matches to train on
        self.config = config  # Configuration file
        self.frame_count = 0

    def start_training(self):
        """
        Use the most recent version of the network to generate the trainings data and save it into the replay buffer
        """
        p = Process(target=self.train_network, args=[])
        p.start()
        num_cores = multiprocessing.cpu_count() - 4
        pool_selfplay = Pool(processes=num_cores)
        pool_selfplay.apply_async(self.run_selfplay())
        p.join()

    def run_selfplay(self):
        """
        Each of the parallel processes is continuously generating new training data
        """
        print("Selfplay worker started")
        while True:
            network = self.network_storage.latest_network()
            game = self.play_game(network)
            self.replay_buffer.save_game(game)
            if len(self.replay_buffer) > self.config.buffer_save_game_interval:
                self.replay_buffer.save()


    def play_game(self, network: Network) -> Game:
        """
        Each of the games produced by a process is played using the latest network,
        so that quality of training data should increase very quick at the beginning
        """
        player_helper = 0
        game = self.config.new_game()
        tree = Tree(action_list=game.legal_actions(),
                    config=self.config,
                    network=network,
                    player_list=game.players,
                    discount=self.config.discount)


        while not game.terminal() and len(game.root_values) < self.config.max_moves:
            self.frame_count += 1
            image = game.make_image(-1)
            value, reward, policy_logits, hidden_state = network.initial_inference(image)

            tree.reset(value=value, reward=reward, policy_logits=policy_logits, hidden_state=hidden_state)
            action = tree.get_action(evaluation=False)

            game.apply(action, Player(player_helper % len(game.players)))
            game.store_search_statistics(tree.root)
        return game

    def train_network(self):
        """
        Create a new network and train it on the data generated by the self-play

        :return: A integer to make it possible to await this method
        """
        print("Started training job but waits till buffer is filled enough")
        # Create new network (representation model, dynamics model, prediction model)
        network = Network(num_action=self.config.action_space_size, game_mode='Atari')
        # Set the learning rate accordingly to the current training step
        learning_rate = self.config.lr_init * self.config.lr_decay_rate ** (
                network.train_step / self.config.lr_decay_steps)
        # Optimizer is the SGD optimizer with momentum
        # FIXME: optimizer = SGD(learning_rate, self.config.momentum)

        while len(self.replay_buffer.buffer) < self.config.batch_size:
            time.sleep(5)
            if os.path.isfile('replay_buffer.npz'):
                self.replay_buffer.load()
        print("Training started")

        for step in range(self.config.training_steps):
            # Save the current state of the network
            if step % self.config.checkpoint_interval == 0:
                self.network_storage.save_network(step, network)

            # Sample a batch from the replay buffer
            batch = self.replay_buffer.sample_batch(self.config.num_unroll_steps, self.config.td_steps)

            # Calculate the loss that results from the batch and update the weights accordingly
            self.update_weights(network, batch, self.config.weight_decay)
            print("Trained one step")

        # Finally save the trained network
        self.network_storage.save_network(self.config.training_steps, network)

    def update_weights(self, network: Network, batch, weight_decay: float):
        """
        First predict the values for the given observations and actions
        Then calculate the loss between those predictions and results of the MCTS stored in the replay buffer
        Before optimizing the weights, add a L2-Regularization to the loss (adds a penalty if the weights get to big)
        Finally updating all weights in the network (all 3 models) with the given optimizer.
        """
        loss = 0
        for image, actions, targets in batch:
            # Transform real observation to hidden state, then predict value, reward and policy distribution for it
            # TODO: Dont understand how the reward can be predicted on a single state (without any given action)
            value, reward, policy_logits, hidden_state = network.initial_inference(image)
            predictions = [(1.0, value, reward, policy_logits)]

            # Recurrent steps, from action and previous hidden state.
            for action in actions:
                value, reward, policy_logits, hidden_state = network.recurrent_inference(hidden_state, action)
                hidden_state = scale_gradient(hidden_state, 0.5)
                predictions.append((1.0 / len(actions), value, reward, policy_logits))

            # Calculate the loss between the predictions and the target
            for prediction, target in zip(predictions, targets):
                gradient_scale, value, reward, policy_logits = prediction
                target_value, target_reward, target_policy = target
                l = (
                        self.scalar_loss(value, target_value) +
                        self.scalar_loss(reward, target_reward) +
                        self.scalar_loss(policy_logits, target_policy)
                )

                loss += scale_gradient(l, gradient_scale)

        # Add L2-Regularization to the overall loss
        for weights in network.get_weights():
            loss += weight_decay * tf.nn.l2_loss(weights)

        # Optimize the current weights
        print('Loss: ', loss)
        def get_loss():
            return loss

        network.minimize_loss(get_loss)
        network.train_step += 1

    def scalar_loss(self, prediction, target) -> float:
        """
        Calculating the scalar loss between prediction and the target and returning it as a tensor.
        MSE in board games
        TODO: Finish cross entropy for atari games with categorical values
        """
        if isinstance(self.config, MuZeroBoardConfig):
            squared_error = tf.keras.losses.MSE(np.array([prediction]), np.array[target])
            return tf.cast(squared_error, dtype=float)

        elif isinstance(self.config, MuZeroAtariConfig):
            squared_error = tf.keras.losses.MSE(np.array([prediction]), np.array([target]))
            return tf.cast(squared_error, dtype=float)
        else:
            raise Exception('MuZero', 'Please use the AtariConfig or the BoardConfig for calculating appropriate loss')

    def evaluate(self):
        pass


def scale_gradient(tensor, scale):
    return tensor * scale + tf.stop_gradient(tensor) * (1 - scale)
